{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6yy-HZ1gY_o"
      },
      "source": [
        "# Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tip: Cree su propia función para procesar el RDD leído.\n",
        "\n",
        "Recursos de esta clase\n",
        "data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00OuUwhdft3J"
      },
      "source": [
        "1. Cree un RDD llamado lenguajes que contenga los siguientes lenguajes de programación: Python, R, C, Scala, Rugby y SQL.\n",
        "\n",
        "1.1 Cree su propia función para procesar el RDD leído.\n",
        "\n",
        "a. Obtenga un nuevo RDD a partir del RDD lenguajes donde todos los lenguajes de programación estén en mayúsculas.\n",
        "\n",
        "\n",
        "\n",
        "b. Obtenga un nuevo RDD a partir del RDD lenguajes donde todos los lenguajes de programación estén en minúsculas.\n",
        "\n",
        "\n",
        "\n",
        "c. Cree un nuevo RDD que solo contenga aquellos lenguajes de programación que comiencen con la letra R.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Configuración contexto Spark\n",
        "conf = SparkConf().setAppName(\"EjercicioRDD\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crea el RDD con los lenguajes de programación\n",
        "lenguajes = sc.parallelize([\"Python\", \"R\", \"C\", \"Scala\", \"Rugby\", \"SQL\"])\n",
        "\n",
        "# Define una función para procesar el RDD\n",
        "def procesar_rdd(rdd):\n",
        "    rdd_mayusculas = rdd.map(lambda x: x.upper())\n",
        "    rdd_minusculas = rdd.map(lambda x: x.lower())\n",
        "    rdd_con_R = rdd.filter(lambda x: x.startswith(\"R\"))\n",
        "    return rdd_mayusculas, rdd_minusculas, rdd_con_R\n",
        "\n",
        "# Procesa el RDD\n",
        "rdd_mayusculas, rdd_minusculas, rdd_con_R = procesar_rdd(lenguajes)\n",
        "\n",
        "# Muestra los resultados\n",
        "print(\"Lenguajes en mayúsculas:\", rdd_mayusculas.collect())\n",
        "print(\"Lenguajes en minúsculas:\", rdd_minusculas.collect())\n",
        "print(\"Lenguajes que comienzan con R:\", rdd_con_R.collect())\n",
        "\n",
        "# Detiene el contexto Spark\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Cree un RDD llamado pares que contenga los números pares existentes en el intervalo [20;30].\n",
        "\n",
        "a. Cree el RDD llamado sqrt, este debe contener la raíz cuadrada de los elementos que componen el RDD pares.\n",
        "b. Obtenga una lista compuesta por los números pares en el intervalo [20;30] y sus respectivas raíces cuadradas. Un ejemplo del resultado deseado para el intervalo [50;60] sería la lista [50, 7.0710678118654755, 52, 7.211102550927978, 54, 7.3484692283495345, 56, 7.483314773547883, 58, 7.615773105863909, 60, 7.745966692414834].\n",
        "c.Eleve el número de particiones del RDD sqrt a 20.\n",
        "d. Si tuviera que disminuir el número de particiones luego de haberlo establecido en 20, ¿qué función utilizaría para hacer más eficiente su código?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import math\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "\n",
        "# Configuración para el contexto Spark\n",
        "conf = SparkConf().setAppName(\"EjercicioRDD\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crea el RDD con los números pares en el intervalo [20;30]\n",
        "pares = sc.parallelize(range(20, 31, 2))\n",
        "\n",
        "# Calcula la raíz cuadrada de los elementos en el RDD pares\n",
        "sqrt = pares.map(lambda x: math.sqrt(x))\n",
        "\n",
        "# Combina los elementos del RDD pares y sqrt en una lista\n",
        "pares_y_sqrt = pares.zip(sqrt).flatMap(lambda x: x)\n",
        "\n",
        "# Aumenta el número de particiones del RDD sqrt a 20\n",
        "sqrt = sqrt.repartition(20)\n",
        "\n",
        "# Muestra los resultados\n",
        "print(\"Números pares y sus raíces cuadradas:\")\n",
        "print(pares_y_sqrt.collect())\n",
        "\n",
        "# Detiene el contexto Spark\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Cree un RDD del tipo clave valor a partir de los datos adjuntos como recurso a esta lección. \n",
        "Tenga en cuenta que deberá procesar el RDD leído para obtener el resultado solicitado. Supongamos que el RDD resultante de tipo clave valor refleja las transacciones realizadas por número de cuentas. \n",
        "Obtenga el monto total por cada cuenta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Crea configuración para el contexto Spark\n",
        "conf = SparkConf().setAppName(\"EjercicioRDD\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Carga los datos como un RDD\n",
        "data_rdd = sc.textFile(\"datos_adjuntos.csv\")  # agregar la ruta\n",
        "\n",
        "# Ignora la primera línea (encabezados)\n",
        "header = data_rdd.first()\n",
        "data_rdd = data_rdd.filter(lambda line: line != header)\n",
        "\n",
        "# Divide cada línea en campos y crea un RDD clave-valor\n",
        "# La clave será el número de cuenta y el valor será el monto\n",
        "key_value_rdd = data_rdd.map(lambda line: line.split(',')).map(lambda fields: (fields[0], float(fields[-1])))\n",
        "\n",
        "# Calcula el monto total por cada número de cuenta\n",
        "monto_total_por_cuenta = key_value_rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Muestra los resultados\n",
        "print(\"Monto total por número de cuenta:\")\n",
        "print(monto_total_por_cuenta.collect())\n",
        "\n",
        "# Detiene el contexto Spark\n",
        "sc.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
