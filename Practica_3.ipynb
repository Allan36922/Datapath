{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMOe12yWlm8K"
      },
      "source": [
        "# Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recursos de esta clase\n",
        "data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN4kJcUUltXO"
      },
      "source": [
        "\n",
        "1. Cree un RDD llamado importes a partir del archivo adjunto a esta lección como recurso.\n",
        "a. ¿Cuántos registros tiene el RDD importes?\n",
        "b. ¿Cuál es el valor mínimo y máximo del RDD importes?\n",
        "c. Cree un RDD top15 que contenga los 15 mayores valores del RDD importes. Tenga en cuenta que pueden repetirse los valores. Por último, escriba el RDD top15 como archivo de texto en la carpeta data/salida.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Configuración contexto Spark\n",
        "conf = SparkConf().setAppName(\"EjercicioRDD\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Carga el archivo de números aleatorios como un RDD\n",
        "importes_rdd = sc.textFile(\"rdd.txt\")  # Asegúrate de reemplazar \"rdd.txt\" con la ruta correcta\n",
        "\n",
        "# a. Calcula el número de registros en el RDD importes\n",
        "num_registros = importes_rdd.count()\n",
        "print(\"Número de registros en el RDD importes:\", num_registros)\n",
        "\n",
        "# b. Calcula el valor mínimo y máximo del RDD importes\n",
        "importes_float_rdd = importes_rdd.map(float)\n",
        "valor_minimo = importes_float_rdd.min()\n",
        "valor_maximo = importes_float_rdd.max()\n",
        "print(\"Valor mínimo del RDD importes:\", valor_minimo)\n",
        "print(\"Valor máximo del RDD importes:\", valor_maximo)\n",
        "\n",
        "# c. Crea un RDD top15 con los 15 mayores valores del RDD importes\n",
        "top15 = importes_float_rdd.top(15)\n",
        "\n",
        "# Guarda el RDD top15 como archivo de texto en la carpeta data/salida\n",
        "top15_rdd = sc.parallelize(top15)\n",
        "top15_rdd.saveAsTextFile(\"data/salida/top15\")\n",
        "\n",
        "# Detiene el contexto Spark\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Cree una función llamada factorial que calcule el factorial de un número dado como parámetro. Utilice RDDs para el cálculo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Configuración contexto Spark\n",
        "conf = SparkConf().setAppName(\"EjercicioRDD\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Define una función para calcular el factorial de un número\n",
        "def factorial(n):\n",
        "    if n == 0 or n == 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return n * factorial(n - 1)\n",
        "\n",
        "# Convierte la función en una función distribuida de Spark\n",
        "factorial_rdd = sc.parallelize([5, 6, 7, 8])  # ** agregar la lista de números para calcular los factoriales\n",
        "\n",
        "resultados = factorial_rdd.map(factorial)\n",
        "\n",
        "# Muestra los resultados\n",
        "print(\"Factoriales calculados:\")\n",
        "print(resultados.collect())\n",
        "\n",
        "# Detiene el contexto Spark\n",
        "sc.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
